# Family Audit Chatbot

## Usage Scenario
The main purpose of the chatbot is to intermediate the human-based dialog around the definition of the Family Audit actions and plans. The chatbot relies on the LLM to support the dialog and manages the reference documents to drive the discussion.

## Prerequisities

> [!IMPORTANT]  
> To use the API in production mode you need a OpenAI-compatible server running. The parameters of the server are defined by
> 
> - ``openai_base_url`` parameter: endpoint of the server
> - ``openai_key`` parameter: API key for the server (may not be empty string)
> - ``openai_model`` parameter: name of the model to call (defaults to 'aixpa')

For the scenario, the LLM server may be deployed using the KubeAI framework. 

### Standalone KubeAI Deployment

To deploy the model in a standalone KubeAI environment, use the following Model spec:

```yaml
apiVersion: kubeai.org/v1
kind: Model
metadata:
  name: llama-3.1
spec:
  features: [TextGeneration]
  owner: meta-llama
  url: hf://unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit
  adapters: # <--
  - name: famiglia
    url: hf://Stefano-M/aixpa_amicifamiglia_short_prompt
  engine: VLLM
  args:
  - --load-format=bitsandbytes
  - --max-lora-rank=32
  - --enable-prefix-caching
  - --max-model-len=8192
  resourceProfile: 1xa100:1
  minReplicas: 1
  maxReplicas: 1
```

Change the ``resourceProfile`` value with the values that correspond to the environment.

Once deployed, the following parameters may be used by the chatbot:

- ``openai_base_url``: ``http://$KUBEAI_ENDPOINT/openai/v1``
- ``openai_key``: any value (kubeai does not control the token)
- ``openai_model``: adapter model name ``llama-3.1_famiglia`` (the convention is ``<modelname>_<adaptername>``)
- ``openai_base_model``: base model name (may be the same)

### Deploy in AIxPA platform

To deploy the model in AIxPA platform, use the ``kubeai`` runtime. Specifically,

1. Define the KubeAI-based function:

```python
llm_function = project.new_function("famigliallm",
                                    kind="kubeai-text",
                                    model_name="llama-3.1",
                                    url="hf://unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit",
                                    engine="VLLM",
                                    features=["TextGeneration"],
                                    adapters=[{"name": "famiglia", "url": "hf://Stefano-M/aixpa_amicifamiglia_short_prompt"}])
```

2. Deploy the model:
```python
llm_run = llm_function.run(action="serve",
                           profile="1xa100",
                           args=["--load-format=bitsandbytes", "--max-lora-rank=32", "--enable-prefix-caching", "--max-model-len=8192"])
```

Once deployed, the following parameters may be used by the chatbot:

- ``openai_base_url``: ``http://kubeai/openai/v1``
- ``openai_key``: any value (kubeai does not control the token)
- ``openai_model``: ``llama-3.1-<runid>_famiglia`` (the convention is ``<modelname>_<adaptername>``)
- ``data_artifact``: reference to artifact with the documents to use for RAG (optional if storage artifact is used)
- ``storage_artifact``:  reference to the prepaired vector storage artifact (optional if data artifact is provided)
- 

Where ``runid`` is the ID of the serving run generated by the platform.

## Implementation Details

To build image run:

```docker build -t aixpa-chatbot-api .``` 

Default port for the server is 8018, can be changed in the Dockerfile.

To run locally

`python start_api.py --host 0.0.0.0 --port 8018 --openai_base_url=http://localhost:1235/v1 --openai_key=123 --openai_model=aixpa --openai_base_model=aixpa`

In this case, ensure the ``RAG_documents`` folder is present with the text files representing the plans to be used for RAG.

To run locally as mock API

`python start_api.py --host 0.0.0.0 --port 8018 --mock`


To ensure everyting is working locally, run 

`python test_api.py --host 0.0.0.0 --port 8018`

The espected result is:


> Testing ground retrieval..................**PASSED** <br>
> Testing next turn generation...........**PASSED** <br>
> Testing next turn stream.................**PASSED**

To run docker use the following command:

```docker run -p 8018:8018 -v RAG_documents:/code/RAG_documents aixpa-chatbot-api --openai_base_url=http://localhost:1235/v1 --openai_key=123 --openai_model=aixpa --openai_base_model=aixpa```

with appropriate values for model endpoint and api key.

# Endpoints

There are 3 endpoints available. Two for generation and one to identify in the documents the relevant parts to for the dialogue.


## ```/turn_generation```
Generates the nex turn of the dialogue.
POST request taking in input a json with: 

`user`: who is the user interacting with the chatbot, either  ***cittadino*** or ***operatore***

`tone`: the tone of the conversation, either ***formale*** or ***informale***

`documents_list`: list of the document(s) on witch the chat is based (containtng the full texts).

`dialogue_list`: list of jsons with the previous turns. Each turn contains the speaker and the text of the turn.

`chatbot_is_first`: Boolean. Indicate if the first turn is from the chatbot or from the user.

Below an example of the json


```json
{
    "user": "cittadino|operatore",
    "tone": "formale|informale",
    "documents_list": [
      "text of document 1",
      "text of document 2",
      "text of document 3"
    ],
    "dialogue_list":[
        {
        "speaker":"operatore",
        "turn_text":"turn text"
        },
        {
        "speaker":"assistant",
        "turn_text":"turn text"
        },
        {
        "speaker":"operatore",
        "turn_text":"turn text"
        }
    ],
    "chatbot_is_first": false
}
```

Returns a json with the message

```json
{
  "turn_text": "text of the next turn"
}
```

## ```/turn_stream```

Generates the nex turn of the dialogue.
POST request, the input is the same of  ```/turn_generation``` 

The output is a data stream.


## ```/turn_ground```

Given a text (e.g. the question of the user or the answer of the chatbot), retrieve from the documents the N most relevant chunks of text.

POST request taking in input a json with: 

`documents_list`: list of the document(s) from witch retrieve the chunks.

`query`: the answer from `/turn_generation` or `/turn_stream` to be grounded

`options_number`: max number of text chunks to retrieve

```json
{
    "documents_list": [
      "text of document 1",
      "text of document 2",
      "text of document 3"
    ],
  "query": "answer to be grounded",
  "options_number": int

}
```

Returns a json with a list of text, each associated with a  `file_index` (index in the `documents_list` from the input) annd the characters offsets.

```json
[
    {
        "text": "retrieved text 1",
        "file_index": 1,
        "offset_start": 0,
        "offset_end": 50
    },
    {
        "text": "retrieved text 2",
        "file_index": 0,
        "offset_start": 30,
        "offset_end": 80
    },
    {
        "text": "retrieved text 3",
        "file_index": 2,
        "offset_start": 240,
        "offset_end": 290
    }
]
```

